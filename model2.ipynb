{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5uxPXt0wquJQ",
        "outputId": "712a341b-a9f0-41d3-baca-18d12c1d8b54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lxml_html_clean\n",
            "  Downloading lxml_html_clean-0.4.2-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from lxml_html_clean) (5.4.0)\n",
            "Downloading lxml_html_clean-0.4.2-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: lxml_html_clean\n",
            "Successfully installed lxml_html_clean-0.4.2\n",
            "Collecting newspaper\n",
            "  Downloading newspaper-0.1.0.7.tar.gz (176 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.9/176.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m2400/2400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 11ms/step - accuracy: 0.7995 - loss: 0.5446 - val_accuracy: 0.9057 - val_loss: 0.2860\n",
            "Epoch 2/5\n",
            "\u001b[1m2400/2400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 11ms/step - accuracy: 0.9162 - loss: 0.2604 - val_accuracy: 0.9034 - val_loss: 0.2853\n",
            "Epoch 3/5\n",
            "\u001b[1m2400/2400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 11ms/step - accuracy: 0.9264 - loss: 0.2167 - val_accuracy: 0.9024 - val_loss: 0.2870\n",
            "Epoch 4/5\n",
            "\u001b[1m2400/2400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 11ms/step - accuracy: 0.9376 - loss: 0.1832 - val_accuracy: 0.9003 - val_loss: 0.2922\n",
            "Epoch 5/5\n",
            "\u001b[1m2400/2400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 11ms/step - accuracy: 0.9425 - loss: 0.1609 - val_accuracy: 0.9038 - val_loss: 0.3223\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.8981 - loss: 0.3328\n",
            "\n",
            "✅ Test Accuracy: 0.90\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step\n",
            "\n",
            "📰 Title:\n",
            " SpaceX rocket being tested in Texas explodes, but no injuries reported\n",
            "\n",
            "📂 Predicted Category: Sci/Tech\n",
            "\n",
            "📝 Summary:\n",
            " A SpaceX rocket being tested in Texas exploded Wednesday (June 18, 2025) night, sending a dramatic fireball high into the sky. It asked people not to try to approach the site.\n"
          ]
        }
      ],
      "source": [
        "# =================== INSTALL REQUIRED LIBRARIES ===================\n",
        "\n",
        "#!pip install pandas nltk scikit-learn keras tensorflow newspaper3k networkx\n",
        "!pip install lxml_html_clean\n",
        "!pip install newspaper\n",
        "# # =================== IMPORTS ===================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import networkx as nx\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from newspaper import Article\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# =================== TEXT CLEANING ===================\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [t for t in tokens if t.isalpha()]\n",
        "    tokens = [t for t in tokens if t not in stop_words]\n",
        "    tokens = [stemmer.stem(t) for t in tokens]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# =================== LOAD & PROCESS DATA ===================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Data/train.csv\")\n",
        "df.columns = ['class_id', 'title', 'description']\n",
        "\n",
        "category_map = {\n",
        "    1: 'World',\n",
        "    2: 'Sports',\n",
        "    3: 'Business',\n",
        "    4: 'Sci/Tech'\n",
        "}\n",
        "df['category'] = df['class_id'].map(category_map)\n",
        "df['text'] = df['title'].fillna('') + \". \" + df['description'].fillna('')\n",
        "df['cleaned'] = df['text'].apply(clean_text)\n",
        "\n",
        "# =================== TOKENIZATION ===================\n",
        "max_words = 5000\n",
        "max_len = 200\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(df['cleaned'])\n",
        "sequences = tokenizer.texts_to_sequences(df['cleaned'])\n",
        "X = pad_sequences(sequences, maxlen=max_len)\n",
        "\n",
        "# =================== LABEL ENCODING ===================\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(df['category'])\n",
        "y = to_categorical(y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# =================== LSTM MODEL ===================\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=max_words, output_dim=64, input_length=max_len))\n",
        "model.add(LSTM(64, return_sequences=False))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(4, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# =================== TRAINING ===================\n",
        "model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# =================== EVALUATION ===================\n",
        "loss, acc = model.evaluate(X_test, y_test)\n",
        "print(f\"\\n✅ Test Accuracy: {acc:.2f}\")\n",
        "\n",
        "# =================== SUMMARIZATION FUNCTION ===================\n",
        "def summarize(text, top_n=2, max_words=40):\n",
        "    sentences = sent_tokenize(text)\n",
        "    if len(sentences) <= top_n:\n",
        "        return ' '.join(sentences[:top_n])\n",
        "\n",
        "    tfidf = Tokenizer()\n",
        "    tfidf.fit_on_texts(sentences)\n",
        "    tfidf_matrix = np.array([\n",
        "        np.mean([tfidf.word_index.get(w, 0) for w in word_tokenize(s.lower()) if w.isalpha()], dtype=float)\n",
        "        for s in sentences\n",
        "    ]).reshape(-1, 1)\n",
        "\n",
        "    sim_matrix = cosine_similarity(tfidf_matrix)\n",
        "    nx_graph = nx.from_numpy_array(sim_matrix)\n",
        "    scores = nx.pagerank(nx_graph)\n",
        "\n",
        "    ranked = sorted(((scores[i], s, i) for i, s in enumerate(sentences)), reverse=True)\n",
        "\n",
        "    summary = []\n",
        "    total_words = 0\n",
        "    for _, sentence, idx in sorted(ranked[:len(sentences)], key=lambda x: x[2]):\n",
        "        word_count = len(sentence.split())\n",
        "        if total_words + word_count <= max_words:\n",
        "            summary.append(sentence)\n",
        "            total_words += word_count\n",
        "        if len(summary) >= top_n or total_words >= max_words:\n",
        "            break\n",
        "\n",
        "    return ' '.join(summary)\n",
        "\n",
        "# =================== URL FETCH FUNCTION ===================\n",
        "def fetch_article(url):\n",
        "    article = Article(url)\n",
        "    article.download()\n",
        "    article.parse()\n",
        "    return article.title, article.text\n",
        "\n",
        "# =================== PREDICT FUNCTION ===================\n",
        "def infer_news_category_and_summary_dl(url):\n",
        "    title, text = fetch_article(url)\n",
        "    cleaned = clean_text(title + \". \" + text)\n",
        "    seq = tokenizer.texts_to_sequences([cleaned])\n",
        "    padded = pad_sequences(seq, maxlen=max_len)\n",
        "\n",
        "    prediction = model.predict(padded)\n",
        "    category = le.inverse_transform([np.argmax(prediction)])[0]\n",
        "    summary = summarize(text)\n",
        "    return {\n",
        "        \"title\": title,\n",
        "        \"category\": category,\n",
        "        \"summary\": summary\n",
        "    }\n",
        "\n",
        "# =================== TEST URL ===================\n",
        "url = \"https://www.thehindu.com/news/international/spacex-rocket-being-tested-in-texas-explodes-but-no-injuries-reported/article69712200.ece\"\n",
        "result = infer_news_category_and_summary_dl(url)\n",
        "\n",
        "print(\"\\n📰 Title:\\n\", result['title'])\n",
        "print(\"\\n📂 Predicted Category:\", result['category'])\n",
        "print(\"\\n📝 Summary:\\n\", result['summary'])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nS7cHik0rTov"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}