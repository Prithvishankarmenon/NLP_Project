{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7d0351",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "!pip install newspaper3k scikit-learn nltk networkx joblib\n",
    "!pip install lxml_html_clean\n",
    "!pip install newspaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464186e8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from joblib import dump, load\n",
    "\n",
    "from newspaper import Article\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "\n",
    "# Download NLTK resources if not already done\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize stopwords and stemmer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Clean text function: keeps only sentences with at least 8 cleaned words\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    sentences = sent_tokenize(text)\n",
    "    cleaned_sentences = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tokens = word_tokenize(sentence)\n",
    "        tokens = [t for t in tokens if t.isalpha()]\n",
    "        tokens = [t for t in tokens if t not in stop_words]\n",
    "        tokens = [stemmer.stem(t) for t in tokens]\n",
    "\n",
    "        if len(tokens) >= 8:\n",
    "            cleaned_sentences.append(' '.join(tokens))\n",
    "\n",
    "    return ' '.join(cleaned_sentences)\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"/content/drive/MyDrive/Data/train.csv\")\n",
    "df.columns = ['class_id', 'title', 'description']\n",
    "\n",
    "# Map class IDs to category labels\n",
    "category_map = {\n",
    "    1: 'World',\n",
    "    2: 'Sports',\n",
    "    3: 'Business',\n",
    "    4: 'Sci/Tech'\n",
    "}\n",
    "df['category'] = df['class_id'].map(category_map)\n",
    "\n",
    "# Combine title and description\n",
    "df['text'] = df['title'].fillna('') + \". \" + df['description'].fillna('')\n",
    "\n",
    "# Apply cleaning\n",
    "df['cleaned'] = df['text'].apply(clean_text)\n",
    "\n",
    "# Remove rows with empty cleaned text\n",
    "df = df[df['cleaned'].str.strip() != '']\n",
    "\n",
    "# Show a sample\n",
    "print(df[['category', 'text']].head())\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['cleaned'], df['category'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Pipeline with TF-IDF and Naive Bayes\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=5000)),\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Train model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save model\n",
    "dump(pipeline, \"news_classifier.joblib\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a2b34d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import feedparser\n",
    "from datetime import datetime\n",
    "from newspaper import Article\n",
    "from joblib import load\n",
    "import nltk\n",
    "import networkx as nx\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Download NLTK resources if not present\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load classifier from earlier model\n",
    "clf = load(\"news_classifier.joblib\")\n",
    "\n",
    "# Setup NLP tools\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    sentences = sent_tokenize(text)\n",
    "    cleaned_sentences = []\n",
    "    for sentence in sentences:\n",
    "        tokens = word_tokenize(sentence)\n",
    "        tokens = [t for t in tokens if t.isalpha()]\n",
    "        tokens = [t for t in tokens if t not in stop_words]\n",
    "        tokens = [stemmer.stem(t) for t in tokens]\n",
    "        if len(tokens) >= 8:\n",
    "            cleaned_sentences.append(' '.join(tokens))\n",
    "    return ' '.join(cleaned_sentences)\n",
    "\n",
    "def summarize(text, top_n=2, max_words=40):\n",
    "    sentences = sent_tokenize(text)\n",
    "    if len(sentences) <= top_n:\n",
    "        return ' '.join(sentences)\n",
    "    \n",
    "    tfidf = TfidfVectorizer().fit_transform(sentences)\n",
    "    sim_matrix = cosine_similarity(tfidf)\n",
    "    nx_graph = nx.from_numpy_array(sim_matrix)\n",
    "    scores = nx.pagerank(nx_graph)\n",
    "\n",
    "    ranked = sorted(((scores[i], s, i) for i, s in enumerate(sentences)), reverse=True)\n",
    "    \n",
    "    summary = []\n",
    "    total_words = 0\n",
    "    for _, sentence, idx in sorted(ranked, key=lambda x: x[2]):\n",
    "        word_count = len(sentence.split())\n",
    "        if total_words + word_count <= max_words:\n",
    "            summary.append(sentence)\n",
    "            total_words += word_count\n",
    "        if len(summary) >= top_n or total_words >= max_words:\n",
    "            break\n",
    "    return ' '.join(summary)\n",
    "\n",
    "def fetch_article_details(url):\n",
    "    try:\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        title, text = article.title, article.text\n",
    "        cleaned = clean_text(title + \". \" + text)\n",
    "        predicted_category = clf.predict([cleaned])[0]\n",
    "        summary = summarize(text)\n",
    "        return {\n",
    "            \"title\": title,\n",
    "            \"url\": url,\n",
    "            \"predicted_category\": predicted_category,\n",
    "            \"summary\": summary\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def fetch_top_articles_from_rss(date_str, category_rss_path):\n",
    "    url = f\"https://www.thehindu.com/{category_rss_path}/?service=rss\"\n",
    "    feed = feedparser.parse(url)\n",
    "    target_date = datetime.strptime(date_str, \"%Y-%m-%d\").date()\n",
    "\n",
    "    articles = []\n",
    "    for entry in feed.entries:\n",
    "        if hasattr(entry, 'published_parsed'):\n",
    "            pub_date = datetime(*entry.published_parsed[:6]).date()\n",
    "            if pub_date == target_date:\n",
    "                details = fetch_article_details(entry.link)\n",
    "                if details:\n",
    "                    articles.append(details)\n",
    "        if len(articles) >= 5:\n",
    "            break\n",
    "    return articles\n",
    "\n",
    "def main():\n",
    "    date_input = input(\"ðŸ“… Enter date (YYYY-MM-DD): \").strip()\n",
    "\n",
    "    category_mapping = {\n",
    "        \"World\": \"news/international\",\n",
    "        \"Business\": \"business\",\n",
    "        \"Sports\": \"sport\",\n",
    "        \"Sci/Tech\": \"sci-tech\"\n",
    "    }\n",
    "\n",
    "    final_results = {}\n",
    "\n",
    "    for category_name, rss_path in category_mapping.items():\n",
    "        print(f\"\\nðŸ” Fetching top 5 articles for **{category_name}** on {date_input}...\")\n",
    "        articles = fetch_top_articles_from_rss(date_input, rss_path)\n",
    "        final_results[category_name] = articles\n",
    "\n",
    "        for idx, article in enumerate(articles, 1):\n",
    "            print(f\"\\n{idx}. ðŸ“° Title: {article['title']}\")\n",
    "            print(f\"   ðŸ”— URL: {article['url']}\")\n",
    "            print(f\"   ðŸ“‚ Predicted Category: {article['predicted_category']}\")\n",
    "            print(f\"   ðŸ“ Summary:\\n{article['summary'][:300]}{'...' if len(article['summary']) > 300 else ''}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
